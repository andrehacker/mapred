/*
 * See http://svn.apache.org/viewvc/hadoop/common/tags/release-1.0.4/src/examples/org/apache/hadoop/examples/WordCount.java?view=markup for WordCount example delivered with Hadoop
 * TODO: Process generic options
 */

package de.andrehacker;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


/*
  Why an outer class?
*/
public class TestAndre extends Configured implements Tool {
    
    /*
      Wrapper class for MAP
    */
    public static class Map extends Mapper<Object, Text, Text, IntWritable> {
	private final static IntWritable one = new IntWritable(1);
	private Text word = new Text();

	public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
	    String line = value.toString();
	    StringTokenizer tokenizer = new StringTokenizer(line);
	    while (tokenizer.hasMoreTokens()) {
		word.set(tokenizer.nextToken());
		context.write(word, one);
	    }
	}
    }

    /*
      class for Reduce
    */
    public static class Reduce extends Reducer<Text, IntWritable, Text, IntWritable> {
	
	private IntWritable result = new IntWritable();

	public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
	    int sum = 0;
	    for (IntWritable val : values) {
		sum += val.get();
	    }
	    result.set(sum);
	    context.write(key, result);
	}

    }
    
    
    @Override
    public int run(String[] args) throws Exception {
	if (args.length != 2) {
	    System.err.printf("Usage: %s [generic options] <input> <output>", getClass().getSimpleName());
	    ToolRunner.printGenericCommandUsage(System.err);
	    return -1;
	}

	Job job = new Job(getConf(), "Andres Wordcount");
	job.setJarByClass(getClass());

	// Key/Value Type
	job.setOutputKeyClass(Text.class);
	job.setOutputValueClass(IntWritable.class);

	job.setMapperClass(Map.class);
	// Optional: Use local combiner (we have a combinable reducer)
	job.setCombinerClass(Reduce.class);
	job.setReducerClass(Reduce.class);

	// Input/Output Format
	// We use the default.
	// For file based input/output formats we can use the predefined FileInputFormat class
	// It has a generic implementation of getSplits()
	// One map tasks is spawned for each InputSplit generated by the InputFormat
	// see http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html
	//job.setInputFormatClass(TextInputFormat.class);
	//job.setOutputFormatClass(TextOutputFormat.class);

	// configure the used input/output format class.
	FileInputFormat.addInputPath(job, new Path(args[0]));
	FileOutputFormat.setOutputPath(job, new Path(args[1]));

	// Set number of mappers and reducers manually
	// Not done here, will be done via command line parameter
	//conf.setNumMapTasks(4);
	//conf.setNumReduceTasks(2);

	return job.waitForCompletion(true) ? 0 : 1;
    }

    /*
      ...
     */
    public static void main(String[] args) throws Exception {
	
	// ToolRunner will call the run method
	// Internally uses GenericOptionsParser to process generic parameters
	int exitCode = ToolRunner.run(new TestAndre(), args);
	System.exit(exitCode);

    }

}
