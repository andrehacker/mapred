/*
 * See http://svn.apache.org/viewvc/hadoop/common/tags/release-1.0.4/src/examples/org/apache/hadoop/examples/WordCount.java?view=markup for WordCount example delivered with Hadoop
 * TODO: Process generic options
 */

package de.andrehacker;

import java.io.IOException;
import java.util.*;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;
import org.apache.hadoop.util.*;

/*
  Why an outer class?
*/
public class TestAndre {
    
    /*
      Wrapper class for MAP
    */
    public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {
	private final static IntWritable one = new IntWritable(1);
	private Text word = new Text();

	public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {
	    String line = value.toString();
	    StringTokenizer tokenizer = new StringTokenizer(line);
	    while (tokenizer.hasMoreTokens()) {
		word.set(tokenizer.nextToken());
		output.collect(word, one);
	    }
	}
    }

    /*
      class for Reduce
    */
    public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {
	/*
	  REDUCE implementation
	 */
	public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {
	    int sum = 0;
	    while (values.hasNext()) {
		sum += values.next().get();
	    }
	    output.collect(key, new IntWritable(sum));
	}
    }
    
    /*
      ...
     */
    public static void main(String[] args) throws Exception {

	// JobConf class describes the whole job
	//   more options: setNumMapTasks, setOutputKeyComparatorClass...
	JobConf conf = new JobConf(TestAndre.class);
	conf.setJobName("Andres Wordcount");

	// KEY/VALUE TYPE
	conf.setOutputKeyClass(Text.class);
	conf.setOutputValueClass(IntWritable.class);

	conf.setMapperClass(Map.class);
	// Optional: Use local combiner (we have a combinable reducer)
	conf.setCombinerClass(Reduce.class);
	conf.setReducerClass(Reduce.class);
	
	// For file based input/output formats we can use the predefined FileInputFormat class
	// It has a generic implementation of getSplits()
	// One map tasks is spawned for each InputSplit generated by the InputFormat
	// see http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html
	conf.setInputFormat(TextInputFormat.class);
	conf.setOutputFormat(TextOutputFormat.class);

	// Set number of mappers and reducers manually
	// Not done here, will be done via command line parameter
	//conf.setNumMapTasks(4);
	//conf.setNumReduceTasks(2);

	// configure the used input/output format class.
	FileInputFormat.setInputPaths(conf, new Path(args[0]));
	FileOutputFormat.setOutputPath(conf, new Path(args[1]));

	// JobClient is interface to interact with cluster
	// allows to submit jobs, track progress, access reports, get cluster status, ...
	// runJob will create InputSplits, copy the job's jar to hdfs, ...
	// runJob returns after job is finished (synchronous call). Alternative: submitJob(...)
	// See http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/JobClient.html
	JobClient.runJob(conf);
    }

}
